# Test
## Papers
### Foundation Models
1. [2024/10/31] AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents[[paper](https://arxiv.org/abs/2410.24024)]
2. [2024/10/30] OS-ATLAS: A Foundation Action Model for Generalist GUI Agents[[paper](https://arxiv.org/abs/2410.23218)]
3. [2024/10/28] AutoGLM: Autonomous Foundation Agents for GUIs[[paper](https://arxiv.org/abs/2411.00820)]
4. [2024/10/25] EDGE: Enhanced Grounded GUI Understanding with Enriched Multi-Granularity Synthetic Data[[paper](https://arxiv.org/abs/2410.19461)]
5. [2024/10/24] Ferret-UI One: Mastering Universal User Interface Understanding Across Platforms[[paper](https://arxiv.org/abs/2410.18967)]
6. [2024/10/22] ShowUI: One Vision-Language-Action Model for Generalist GUI Agent[[paper](https://openreview.net/forum?id=UXdxYnkJtX)]
7. [2024/10/17] Harnessing Webpage UIs for Text-Rich Visual Understanding[[paper](https://arxiv.org/abs/2410.13824)]
8. [2024/10/09] TinyClick: Single-Turn Agent for Empowering GUI Automation[[paper](https://arxiv.org/abs/2410.11871)]
9. [2024/10/07] Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents[[paper](https://arxiv.org/abs/2410.05243)]
10. [2024/10/07] Navigating The Digital World As Humans Do: Universal Visual Grounding For Gui Agents[[paper](https://arxiv.org/abs/2410.05243)]
11. [2024/10/03] NNetscape Navigator: Complex Demonstrations for Web Agents Without a Demonstrator[[paper](https://arxiv.org/abs/2410.02907)]
12. [2024/09/30] MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning[[paper](https://arxiv.org/abs/2409.20566)]
13. [2024/09/24] Synatra: Turning indirect knowledge into direct demonstrations for digital agents at scale[[paper](https://arxiv.org/abs/2409.15637)]
14. [2024/09/23] MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding[[paper](https://arxiv.org/abs/2409.14818)]
15. [2024/09/22] MobileViews: A Large-Scale Mobile GUI Dataset[[paper](https://arxiv.org/abs/2409.14337)]
16. [2024/08/30] UI-Hawk: Unleashing the screen stream understanding for gui agents[[paper](https://www.preprints.org/manuscript/202408.2137)]
17. [2024/07/19] GUI Action Narrator: Where and When Did That Action Take Place?[[paper](https://arxiv.org/abs/2406.13719)]
18. [2024/07/05] MobileFlow: A Multimodal LLM for Mobile GUI Agent[[paper](https://arxiv.org/abs/2407.04346)]
19. [2024/06/20] VGA: Vision GUI Assistant - Minimizing Hallucinations through Image-Centric Fine-Tuning[[paper](https://arxiv.org/abs/2406.14056)]
20. [2024/06/12] GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices[[paper](https://arxiv.org/abs/2406.08451)]
21. [2024/06/12] Tell Me What's Next: Textual Foresight for Generic UI Representations[[paper](https://arxiv.org/abs/2406.07822)]
22. [2024/05/29] ReALM: Reference Resolution As Language Modeling[[paper](https://arxiv.org/abs/2403.20329)]
23. [2024/05/08] Visual Grounding for User Interfaces[[paper](https://aclanthology.org/2024.naacl-industry.9/)]
24. [2024/05/05] Visual grounding for desktop graphical user interfaces[[paper](https://arxiv.org/abs/2407.01558)]
25. [2024/05/05] Android in the Zoo：Chain-of-Action-Thought for GUI Agents[[paper](https://arxiv.org/abs/2403.02713)]
26. [2024/05/01] Navigating WebAI: Training Agents to Complete Web Tasks with Large Language Models and Reinforcement Learning[[paper](https://arxiv.org/abs/2405.00516)]
27. [2024/04/16] Search Beyond Queries: Training Smaller Language Models for Web Interactions via Reinforcement Learning[[paper](https://arxiv.org/abs/2404.10887)]
28. [2024/04/12] Training a Vision Language Model as Smartphone Assistant[[paper](https://arxiv.org/abs/2404.08755)]
29. [2024/04/09] Autonomous Evaluation and Refinement of Web Agents[[paper](https://arxiv.org/abs/2404.06474)]
30. [2024/04/08] Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs[[paper](https://arxiv.org/abs/2404.05719)]
31. [2024/04/04] AutoWebGLM: A Large Language Model-based Web Navigating Agent[[paper](https://arxiv.org/abs/2404.03648)]
32. [2024/04/04] AutoWebGLM: A Large Language Model-based Web Navigating Agent[[paper](https://arxiv.org/abs/2404.03648)]
33. [2024/04/02] Octopus v2: On-device language model for super agent[[paper](https://arxiv.org/abs/2404.01744)]
34. [2024/03/30] Large Language Models Can Self-Improve At Web Agent Tasks[[paper](https://arxiv.org/abs/2405.20309v2)]
35. [2024/02/08] WebLINX: Real-World website navigation with Multi-Turn dialogue[[paper](https://arxiv.org/abs/2402.05930)]
36. [2024/02/07] ScreenAI: A Vision-Language Model for UI and Infographics Understanding[[paper](https://arxiv.org/abs/2402.04615)]
37. [2024/02/06] Dual-View Visual Contextualization for Web Navigation[[paper](https://arxiv.org/abs/2402.04476)]
38. [2024/01/20] E-ANT: A Large-Scale Dataset for Efficient Automatic GUI NavigaTion[[paper](https://arxiv.org/abs/2406.14250)]
39. [2024/01/17] SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents[[paper](https://arxiv.org/abs/2401.10935)]
40. [2024/01/17] GUICourse: From General Vision Language Models to Versatile GUI Agents[[paper](https://arxiv.org/abs/2406.11317)]
41. [2023/12/25] WebVLN: Vision-and-Language Navigation on Websites[[paper](https://arxiv.org/abs/2312.15820)]
42. [2023/12/25] UINav: A Practical Approach to Train On-Device Automation Agents[[paper](https://aclanthology.org/2024.naacl-industry.4/)]
43. [2023/12/14] CogAgent: A Visual Language Model for GUI Agents[[paper](https://arxiv.org/abs/2312.08914)]
44. [2023/12/04] Intelligent Virtual Assistants with LLM-based Process Automation[[paper](https://arxiv.org/abs/2312.06677)]
45. [2023/11/30] Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web[[paper](https://arxiv.org/abs/2311.18751)]
46. [2023/10/27] Android in the wild: A large-scale dataset for android device control[[paper](https://arxiv.org/abs/2307.10088)]
47. [2023/10/08] UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model[[paper](https://arxiv.org/abs/2310.05126)]
48. [2023/10/07] ILuvUI: Instruction-tuned Language-Vision modeling of UIs from Machine Conversations[[paper](https://arxiv.org/abs/2310.04869)]
49. [2023/10/07] Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API[[paper](https://arxiv.org/abs/2310.04716)]
50. [2023/07/24] A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis[[paper](https://arxiv.org/abs/2307.12856)]
51. [2023/05/31] From pixels to UI actions: Learning to follow instructions via graphical user interfaces[[paper](https://arxiv.org/abs/2306.00245)]
52. [2023/05/19] Multimodal Web Navigation with Instruction-Finetuned Foundation Models[[paper](https://arxiv.org/abs/2305.11854)]
53. [2023/01/30] WebUI: A Dataset for Enhancing Visual UI Understanding with Web Semantics[[paper](https://arxiv.org/abs/2301.13280)]
54. [2023/01/23] Lexi: Self-Supervised Learning of the UI Language[[paper](https://arxiv.org/abs/2301.10165)]
55. [2022/10/06] Towards Better Semantic Understanding of Mobile Interfaces[[paper](https://arxiv.org/abs/2210.02663)]
56. [2022/09/29] Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus[[paper](https://arxiv.org/abs/2209.14927)]
57. [2022/07/04] WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents[[paper](https://arxiv.org/abs/2207.01206)]
58. [2022/05/23] Meta-gui: Towards multi-modal conversational agents on mobile gui[[paper](https://arxiv.org/abs/2205.11029)]
59. [2022/02/16] A data-driven approach for learning to control computers[[paper](https://arxiv.org/abs/2202.08137)]

### Agent Frameworks
### Evaluation & Benchmark
### Safety & Privacy
### RL
1. [2024/10/18] DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agents [[paper](https://arxiv.org/abs/2410.14803)]
2. [2024/06/17] DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning [[paper](https://openreview.net/forum?id=Fqbg7yohJ9)]
3. [2024/05/23] AGILE: A Novel Reinforcement Learning Framework of LLM Agents [[paper](https://arxiv.org/abs/2405.14751)]（cited 0）
4. [2024/04/16] Search Beyond Queries: Training Smaller Language Models for Web Interactions via Reinforcement Learning [[paper](https://arxiv.org/abs/2404.10887)]
5. [2021/05/31] Appbuddy: Learning to accomplish tasks in mobile apps via reinforcement learning [[paper](https://arxiv.org/abs/2106.00133)]
6. [2019/02/19] Dom-q-net: Grounded rl on structured language [[paper](https://arxiv.org/abs/1902.07257)]
7. [2018/12/21] Learning to navigate the web [[paper](https://arxiv.org/abs/1812.09195)] (cited:66)
8. [2018/02/24] Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration [[paper](https://arxiv.org/abs/1802.08802)] (cited: 187)
9. [2017/08/06] World of bits: An open-domain platform for web-based agents [[paper](https://proceedings.mlr.press/v70/shi17a/shi17a.pdf)] (cited:199)
