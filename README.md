# Test

## Table of Contents

- [Test](#test)
  - [Table of Contents](#table-of-contents)
  - [Overview of xxx](#overview-of-xxx)
  - [Papers](#papers)
    - [Foundation Models](#foundation-models)
    - [Agent Frameworks](#agent-frameworks)
    - [Evaluation & Benchmark](#evaluation--benchmark)
    - [Safety & Privacy](#safety--privacy)
  - [Related Repositories](#related-repositories)
  - [Contributing](#contributing)
  - [Citation](#citation)
  - [Acknowledgement](#acknowledgement)

## Overview of xxx

## Papers

### Foundation Models

1. [2024/10/31] AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents[[paper](https://arxiv.org/abs/2410.24024)]
2. [2024/10/30] OS-ATLAS: A Foundation Action Model for Generalist GUI Agents[[paper](https://arxiv.org/abs/2410.23218)]
3. [2024/10/28] AutoGLM: Autonomous Foundation Agents for GUIs[[paper](https://arxiv.org/abs/2411.00820)]
4. [2024/10/25] EDGE: Enhanced Grounded GUI Understanding with Enriched Multi-Granularity Synthetic Data[[paper](https://arxiv.org/abs/2410.19461)]
5. [2024/10/24] Ferret-UI One: Mastering Universal User Interface Understanding Across Platforms[[paper](https://arxiv.org/abs/2410.18967)]
6. [2024/10/22] ShowUI: One Vision-Language-Action Model for Generalist GUI Agent[[paper](https://openreview.net/forum?id=UXdxYnkJtX)]
7. [2024/10/17] Harnessing Webpage UIs for Text-Rich Visual Understanding[[paper](https://arxiv.org/abs/2410.13824)]
8. [2024/10/09] TinyClick: Single-Turn Agent for Empowering GUI Automation[[paper](https://arxiv.org/abs/2410.11871)]
9. [2024/10/07] Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents[[paper](https://arxiv.org/abs/2410.05243)]
10. [2024/10/07] Navigating The Digital World As Humans Do: Universal Visual Grounding For Gui Agents[[paper](https://arxiv.org/abs/2410.05243)]
11. [2024/10/03] NNetscape Navigator: Complex Demonstrations for Web Agents Without a Demonstrator[[paper](https://arxiv.org/abs/2410.02907)]
12. [2024/09/30] MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning[[paper](https://arxiv.org/abs/2409.20566)]
13. [2024/09/24] Synatra: Turning indirect knowledge into direct demonstrations for digital agents at scale[[paper](https://arxiv.org/abs/2409.15637)]
14. [2024/09/23] MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding[[paper](https://arxiv.org/abs/2409.14818)]
15. [2024/09/22] MobileViews: A Large-Scale Mobile GUI Dataset[[paper](https://arxiv.org/abs/2409.14337)]
16. [2024/08/30] UI-Hawk: Unleashing the screen stream understanding for gui agents[[paper](https://www.preprints.org/manuscript/202408.2137)]
17. [2024/07/19] GUI Action Narrator: Where and When Did That Action Take Place?[[paper](https://arxiv.org/abs/2406.13719)]
18. [2024/07/05] MobileFlow: A Multimodal LLM for Mobile GUI Agent[[paper](https://arxiv.org/abs/2407.04346)]
19. [2024/06/20] VGA: Vision GUI Assistant - Minimizing Hallucinations through Image-Centric Fine-Tuning[[paper](https://arxiv.org/abs/2406.14056)]
20. [2024/06/12] GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices[[paper](https://arxiv.org/abs/2406.08451)]
21. [2024/06/12] Tell Me What's Next: Textual Foresight for Generic UI Representations[[paper](https://arxiv.org/abs/2406.07822)]
22. [2024/05/29] ReALM: Reference Resolution As Language Modeling[[paper](https://arxiv.org/abs/2403.20329)]
23. [2024/05/08] Visual Grounding for User Interfaces[[paper](https://aclanthology.org/2024.naacl-industry.9/)]
24. [2024/05/05] Visual grounding for desktop graphical user interfaces[[paper](https://arxiv.org/abs/2407.01558)]
25. [2024/05/05] Android in the Zooï¼šChain-of-Action-Thought for GUI Agents[[paper](https://arxiv.org/abs/2403.02713)]
26. [2024/05/01] Navigating WebAI: Training Agents to Complete Web Tasks with Large Language Models and Reinforcement Learning[[paper](https://arxiv.org/abs/2405.00516)]
27. [2024/04/16] Search Beyond Queries: Training Smaller Language Models for Web Interactions via Reinforcement Learning[[paper](https://arxiv.org/abs/2404.10887)]
28. [2024/04/12] Training a Vision Language Model as Smartphone Assistant[[paper](https://arxiv.org/abs/2404.08755)]
29. [2024/04/09] Autonomous Evaluation and Refinement of Web Agents[[paper](https://arxiv.org/abs/2404.06474)]
30. [2024/04/08] Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs[[paper](https://arxiv.org/abs/2404.05719)]
31. [2024/04/04] AutoWebGLM: A Large Language Model-based Web Navigating Agent[[paper](https://arxiv.org/abs/2404.03648)]
32. [2024/04/04] AutoWebGLM: A Large Language Model-based Web Navigating Agent[[paper](https://arxiv.org/abs/2404.03648)]
33. [2024/04/02] Octopus v2: On-device language model for super agent[[paper](https://arxiv.org/abs/2404.01744)]
34. [2024/03/30] Large Language Models Can Self-Improve At Web Agent Tasks[[paper](https://arxiv.org/abs/2405.20309v2)]
35. [2024/02/08] WebLINX: Real-World website navigation with Multi-Turn dialogue[[paper](https://arxiv.org/abs/2402.05930)]
36. [2024/02/07] ScreenAI: A Vision-Language Model for UI and Infographics Understanding[[paper](https://arxiv.org/abs/2402.04615)]
37. [2024/02/06] Dual-View Visual Contextualization for Web Navigation[[paper](https://arxiv.org/abs/2402.04476)]
38. [2024/01/20] E-ANT: A Large-Scale Dataset for Efficient Automatic GUI NavigaTion[[paper](https://arxiv.org/abs/2406.14250)]
39. [2024/01/17] SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents[[paper](https://arxiv.org/abs/2401.10935)]
40. [2024/01/17] GUICourse: From General Vision Language Models to Versatile GUI Agents[[paper](https://arxiv.org/abs/2406.11317)]
41. [2023/12/25] WebVLN: Vision-and-Language Navigation on Websites[[paper](https://arxiv.org/abs/2312.15820)]
42. [2023/12/25] UINav: A Practical Approach to Train On-Device Automation Agents[[paper](https://aclanthology.org/2024.naacl-industry.4/)]
43. [2023/12/14] CogAgent: A Visual Language Model for GUI Agents[[paper](https://arxiv.org/abs/2312.08914)]
44. [2023/12/04] Intelligent Virtual Assistants with LLM-based Process Automation[[paper](https://arxiv.org/abs/2312.06677)]
45. [2023/11/30] Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web[[paper](https://arxiv.org/abs/2311.18751)]
46. [2023/10/27] Android in the wild: A large-scale dataset for android device control[[paper](https://arxiv.org/abs/2307.10088)]
47. [2023/10/08] UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model[[paper](https://arxiv.org/abs/2310.05126)]
48. [2023/10/07] ILuvUI: Instruction-tuned Language-Vision modeling of UIs from Machine Conversations[[paper](https://arxiv.org/abs/2310.04869)]
49. [2023/10/07] Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API[[paper](https://arxiv.org/abs/2310.04716)]
50. [2023/07/24] A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis[[paper](https://arxiv.org/abs/2307.12856)]
51. [2023/05/31] From pixels to UI actions: Learning to follow instructions via graphical user interfaces[[paper](https://arxiv.org/abs/2306.00245)]
52. [2023/05/19] Multimodal Web Navigation with Instruction-Finetuned Foundation Models[[paper](https://arxiv.org/abs/2305.11854)]
53. [2023/01/30] WebUI: A Dataset for Enhancing Visual UI Understanding with Web Semantics[[paper](https://arxiv.org/abs/2301.13280)]
54. [2023/01/23] Lexi: Self-Supervised Learning of the UI Language[[paper](https://arxiv.org/abs/2301.10165)]
55. [2022/10/06] Towards Better Semantic Understanding of Mobile Interfaces[[paper](https://arxiv.org/abs/2210.02663)]
56. [2022/09/29] Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus[[paper](https://arxiv.org/abs/2209.14927)]
57. [2022/07/04] WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents[[paper](https://arxiv.org/abs/2207.01206)]
58. [2022/05/23] Meta-gui: Towards multi-modal conversational agents on mobile gui[[paper](https://arxiv.org/abs/2205.11029)]
59. [2022/02/16] A data-driven approach for learning to control computers[[paper](https://arxiv.org/abs/2202.08137)]

### Agent Frameworks

1. [2024/11/10] Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents[[paper](https://arxiv.org/abs/2411.06559)]
2. [2024/11/01] WebOlympus: An Open Platform for Web Agents on Live Websites[[paper](https://aclanthology.org/2024.emnlp-demo.20/)]
3. [2024/10/29] Auto-Intent: Automated Intent Discovery and Self-Exploration for Large Language Model Web Agents[[paper](https://arxiv.org/abs/2410.22552)]
4. [2024/10/25] OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization[[paper](https://arxiv.org/abs/2410.19609)]
5. [2024/10/24] OSCAR: Operating System Control via State-Aware Reasoning and Re-Planning[[paper](https://arxiv.org/abs/2410.18963)]
6. [2024/10/24] AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant[[paper](https://arxiv.org/abs/2410.18603)]
7. [2024/10/24] Infogent: An Agent-Based Framework for Web Information Aggregation[[paper](https://arxiv.org/abs/2410.19054)]
8. [2024/10/22] Large Language Models Empowered Personalized Web Agents[[paper](https://arxiv.org/abs/2410.17236)]
9. [2024/10/21] Beyond Browsing: API-Based Web Agents[[paper](https://arxiv.org/abs/2410.16464)]
10. [2024/10/17] AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents[[paper](https://arxiv.org/abs/2410.13825)]
11. [2024/10/17] MobA: A Two-Level Agent System for Efficient Mobile Task Automation[[paper](https://arxiv.org/abs/2410.13757)]
12. [2024/10/11] VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning[[paper](https://dl.acm.org/doi/abs/10.1145/3654777.3676386)]
13. [2024/10/10] Agent S: An Open Agentic Framework that Uses Computers Like a Human[[paper](https://arxiv.org/abs/2410.08164)]
14. [2024/10/09] ClickAgent: Enhancing UI Location Capabilities of Autonomous Agents[[paper](https://arxiv.org/abs/2410.11872)]
15. [2024/10/01] Dynamic Planning for LLM-based Graphical User Interface Automation[[paper](https://arxiv.org/abs/2410.00467)]
16. [2024/10/01] Multimodal Auto Validation For Self-Refinement in Web Agents[[paper](https://arxiv.org/abs/2410.00689)]
17. [2024/09/25] Turn Every Application into an Agent: Towards Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents[[paper](https://arxiv.org/abs/2409.17140)]
18. [2024/09/23] From Commands to Prompts: LLM-based Semantic File System for AIOS[[paper](https://arxiv.org/abs/2410.11843)]
19. [2024/09/23] Steward: Natural Language Web Automation[[paper](https://arxiv.org/abs/2409.15441)]
20. [2024/09/16] NaviQAte: Functionality-Guided Web Application Navigation[[paper](https://arxiv.org/abs/2409.10741)]
21. [2024/09/14] PeriGuru: A Peripheral Robotic Mobile App Operation Assistant based on GUI Image Understanding and Prompting with LLM[[paper](https://arxiv.org/abs/2409.09354)]
22. [2024/09/12] Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale[[paper](https://arxiv.org/abs/2409.08264)]
23. [2024/09/11] Agent Workflow Memory[[paper](https://arxiv.org/abs/2409.07429)]
24. [2024/08/28] Webpilot: A versatile and autonomous multi-agent system for web task execution with strategic exploration[[paper](https://arxiv.org/abs/2408.15978)]
25. [2024/08/24] AutoWebGLM: A Large Language Model-based Web Navigating Agent[[paper](https://dl.acm.org/doi/abs/10.1145/3637528.3671620)]
26. [2024/08/24] Intelligent Agents with LLM-based Process Automation[[paper](https://dl.acm.org/doi/abs/10.1145/3637528.3671646)]
27. [2024/08/01] OpenWebAgent: An Open Toolkit to Enable Web Agents on Large Language Models[[paper](https://aclanthology.org/2024.acl-demos.8/)]
28. [2024/08/01] Omniparser for pure vision based gui agent[[paper](https://arxiv.org/abs/2408.00203)]
29. [2024/07/21] Towards LLMCI: Multimodal AI for LLM-Vision UI Operation[[paper](https://doi.org/10.21203/rs.3.rs-4653823/v1)]
30. [2024/07/17] Agent-e: From autonomous web navigation to foundational design principles in agentic systems[[paper](https://arxiv.org/abs/2407.13032)]
31. [2024/07/04] MobileExperts: A Dynamic Tool-Enabled Agent Team in Mobile Devices[[paper](https://arxiv.org/abs/2407.03913)]
32. [2024/07/01] Tree Search for Language Model Agents[[paper](https://arxiv.org/abs/2407.01476)]
33. [2024/06/27] Read anywhere pointed: Layout-aware gui screen reading with tree-of-lens grounding[[paper](https://arxiv.org/abs/2406.19263)]
34. [2024/06/11] CAAP: Context-Aware Action Planning Prompting to Solve Computer Tasks with Front-End UI Only[[paper](https://arxiv.org/abs/2406.06947)]
35. [2024/06/03] Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration[[paper](https://arxiv.org/abs/2406.01014)]
36. [2024/05/23] AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents[[paper](https://arxiv.org/abs/2405.14573)]
37. [2024/05/17] Latent State Estimation Helps UI Agents to Reason[[paper](https://arxiv.org/abs/2405.11120)]
38. [2024/04/28] MMAC-Copilot: Multi-modal Agent Collaboration Operating System Copilot[[paper](https://arxiv.org/abs/2404.18074)]
39. [2024/04/09] Autonomous Evaluation and Refinement of Web Agents[[paper](https://arxiv.org/abs/2404.06474)]
40. [2024/04/03] PromptRPA: Generating Robotic Process Automation on Smartphones from Textual Prompts[[paper](https://arxiv.org/abs/2404.02475)]
41. [2024/03/25] AIOS: LLM Agent Operating System[[paper](https://arxiv.org/abs/2403.16971v3)]
42. [2024/03/05] Android in the zoo: Chain-of-action-thought for gui agents[[paper](https://arxiv.org/abs/2403.02713)]
43. [2024/03/05] Cradle: Empowering Foundation Agents Towards General Computer Control[[paper](https://arxiv.org/abs/2403.03186v3)]
44. [2024/02/23] On the Multi-turn Instruction Following for Conversational Web Agents[[paper](https://arxiv.org/abs/2402.15057)]
45. [2024/02/19] CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation[[paper](https://arxiv.org/abs/2402.11941)]
46. [2024/02/12] OS-Copilot: Towards Generalist Computer Agents with Self-Improvement[[paper](https://arxiv.org/abs/2402.07456)]
47. [2024/02/09] ScreenAgent: A Vision Language Model-driven Computer Control Agent[[paper](https://arxiv.org/abs/2402.07945)]
48. [2024/02/08] Ufo: A ui-focused agent for windows os interaction[[paper](https://arxiv.org/abs/2402.07939)]
49. [2024/02/06] Dual-View Visual Contextualization for Web Navigation[[paper](https://arxiv.org/abs/2402.04476)]
50. [2024/01/29] Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception[[paper](https://arxiv.org/abs/2401.16158)]
51. [2024/01/25] WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models[[paper](https://arxiv.org/abs/2401.13919)]
52. [2024/01/17] Seeclick: Harnessing gui grounding for advanced visual gui agents[[paper](https://arxiv.org/abs/2401.10935)]
53. [2024/01/04] MobileAgent: enhancing mobile control via human-machine interaction and SOP integration[[paper](https://arxiv.org/abs/2401.04124)]
54. [2024/01/03] GPT-4V(ision) is a Generalist Web Agent, if Grounded[[paper](https://arxiv.org/abs/2401.01614)]
55. [2023/12/21] AppAgent: Multimodal Agents as Smartphone Users[[paper](https://arxiv.org/abs/2312.13771)]
56. [2023/12/20] Assistgui: Task-oriented desktop graphical user interface automation[[paper](https://arxiv.org/abs/2312.13108)]
57. [2023/12/04] MobileGPT: Augmenting LLM with Human-like App Memory for Mobile Task Automation[[paper](https://arxiv.org/abs/2312.03003v3)]
58. [2023/12/04] Intelligent Virtual Assistants with LLM-based Process Automation[[paper](https://arxiv.org/abs/2312.06677)]
59. [2023/11/13] GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation[[paper](https://arxiv.org/abs/2311.07562)]
60. [2023/10/24] WebWISE: Web Interface Control and Sequential Exploration with Large Language Models[[paper](https://arxiv.org/abs/2310.16042)]
61. [2023/10/12] A Zero-Shot Language Agent for Computer Control with Structured Reflection[[paper](https://arxiv.org/abs/2310.08740)]
62. [2023/09/20] You only look at screens: Multimodal chain-of-action agents[[paper](https://arxiv.org/abs/2309.11436)]
63. [2023/09/15] Laser: Llm agent with state-space exploration for web navigation[[paper](https://arxiv.org/abs/2309.08172)]
64. [2023/08/29] AutoDroid: LLM-powered Task Automation in Android[[paper](https://arxiv.org/abs/2308.15272)]
65. [2023/07/24] A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis[[paper](https://arxiv.org/abs/2307.12856)]
66. [2023/06/14] Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control[[paper](https://arxiv.org/abs/2306.07863)]
67. [2023/06/09] Mind2Web: Towards a Generalist Agent for the Web[[paper](https://arxiv.org/abs/2306.06070)]
68. [2023/05/30] Sheetcopilot: Bringing software productivity to the next level through large language models[[paper](https://arxiv.org/abs/2305.19308)]
69. [2023/05/23] Hierarchical prompting assists large language model on web navigation[[paper](https://arxiv.org/abs/2305.14257)]
70. [2023/05/09] InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language[[paper](https://arxiv.org/abs/2305.05662)]
71. [2023/03/30] Language Models can Solve Computer Tasks[[paper](https://arxiv.org/abs/2303.17491)]
72. [2022/09/19] Enabling conversational interaction with mobile ui using large language models[[paper](https://arxiv.org/abs/2209.08655)]
73. [2022/05/23] Meta-gui: Towards multi-modal conversational agents on mobile gui[[paper](https://arxiv.org/abs/2205.11029)]

### Evaluation & Benchmark

1. [2024/10/31] AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents[[paper](https://arxiv.org/abs/2410.24024)]
2. [2024/10/28] AssistEditor: Multi-Agent Collaboration for GUI Workflow Automation in Video Creation[[paper](https://dl.acm.org/doi/abs/10.1145/3664647.3684998)]
3. [2024/10/28] Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large Language Models[[paper](https://arxiv.org/abs/2410.20745)]
4. [2024/10/28] MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI[[paper](https://arxiv.org/abs/2404.16006)]
5. [2024/10/24] VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks[[paper](https://arxiv.org/abs/2410.19100)]
6. [2024/10/22] Large Language Models Empowered Personalized Web Agents[[paper](https://arxiv.org/abs/2410.17236)]
7. [2024/10/19] SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation[[paper](https://arxiv.org/abs/2410.15164)]
8. [2024/10/17] MobA: A Two-Level Agent System for Efficient Mobile Task Automation[[paper](https://arxiv.org/abs/2410.13757)]
9. [2024/10/07] Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents[[paper](https://arxiv.org/abs/2410.05243)]
10. [2024/09/22] MobileViews: A Large-Scale Mobile GUI Dataset[[paper](https://arxiv.org/abs/2409.14337)]
11. [2024/09/12] Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale[[paper](https://arxiv.org/abs/2409.08264)]
12. [2024/09/06] WebQuest: A Benchmark for Multimodal QA on Web Page Sequences[[paper](https://arxiv.org/abs/2409.13711)]
13. [2024/08/01] Omniparser for pure vision based gui agent[[paper](https://arxiv.org/abs/2408.00203)]
14. [2024/07/26] OfficeBench: Benchmarking Language Agents across Multiple Applications for Office Automation[[paper](https://arxiv.org/abs/2407.19056)]
15. [2024/07/22] AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?[[paper](https://arxiv.org/abs/2407.15711)]
16. [2024/07/15] Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?[[paper](https://arxiv.org/abs/2407.10956)]
17. [2024/07/13] RealWeb: A Benchmark for Universal Instruction Following in Realistic Web Services Navigation[[paper](https://ieeexplore.ieee.org/abstract/document/10707522)]
18. [2024/07/11] UICrit: Enhancing Automated Design Evaluation with a UI Critique Dataset[[paper](https://arxiv.org/abs/2407.08850)]
19. [2024/07/07] WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks[[paper](https://arxiv.org/abs/2407.05291)]
20. [2024/07/04] MobileExperts: A Dynamic Tool-Enabled Agent Team in Mobile Devices[[paper](https://arxiv.org/abs/2407.03913)]
21. [2024/07/03] Amex: Android multi-annotation expo dataset for mobile gui agents[[paper](https://arxiv.org/abs/2407.17490)]
22. [2024/07/01] CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents[[paper](https://arxiv.org/abs/2407.01511)]
23. [2024/07/01] Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents[[paper](https://arxiv.org/abs/2407.00993)]
24. [2024/06/27] Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Grounding[[paper](https://arxiv.org/abs/2406.19263)]
25. [2024/06/20] E-ANT: A Large-Scale Dataset for Efficient Automatic GUI NavigaTion[[paper](https://arxiv.org/abs/2406.14250)]
26. [2024/06/20] Identifying User Goals from UI Trajectories[[paper](https://arxiv.org/abs/2406.14314)]
27. [2024/06/19] GUI Action Narrator: Where and When Did That Action Take Place?[[paper](https://arxiv.org/abs/2406.13719)]
28. [2024/06/18] WebCanvas: Benchmarking Web Agents in Online Environments[[paper](https://arxiv.org/abs/2406.12373)]
29. [2024/06/17] GUICourse: From General Vision Language Models to Versatile GUI Agents[[paper](https://arxiv.org/abs/2406.11317)]
30. [2024/06/14] VideoGUI: A Benchmark f om Instructional Videos[[paper](https://arxiv.org/abs/2406.10227)]
31. [2024/06/12] GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices[[paper](https://arxiv.org/abs/2406.08451)]
32. [2024/06/12] GUI-WORLD: A Dataset for GUI-oriented Multimodal LLM-based Agents[[paper](https://arxiv.org/abs/2406.10819)]
33. [2024/06/12] MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents[[paper](https://arxiv.org/abs/2406.08184)]
34. [2024/06/06] On the effects of data scale on computer control agents[[paper](https://arxiv.org/abs/2406.03679)]
35. [2024/06/06] On the Effects of Data Scale on UI Control Agents[[paper](https://arxiv.org/abs/2406.03679)]
36. [2024/06/05] WebOlympus: An Open Platform for Web Agents on Live Websites[[paper](https://aclanthology.org/2024.emnlp-demo.20/)]
37. [2024/06/05] UGIF-DataSet: A New Dataset for Cross-lingual, Cross-modal Sequential actions on the UI[[paper](https://aclanthology.org/2024.findings-naacl.89/)]
38. [2024/06/01] WebSuite: Systematically Evaluating Why Web Agents Fail[[paper](https://arxiv.org/abs/2406.01623)]
39. [2024/05/23] AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents[[paper](https://arxiv.org/abs/2405.14573)]
40. [2024/05/23] AGILE: A Novel Reinforcement Learning Framework of LLM Agents[[paper](https://arxiv.org/abs/2405.14751)]
41. [2024/05/17] Latent state estimation helps ui agents to reason[[paper](https://arxiv.org/abs/2405.11120)]
42. [2024/05/07] Mapping natural language instructions to mobile UI action sequences[[paper](https://arxiv.org/abs/2005.03776)]
43. [2024/05/05] Visual grounding for desktop graphical user interfaces[[paper](https://arxiv.org/abs/2407.01558)]
44. [2024/04/28] MMAC-Copilot: Multi-modal Agent Collaboration Operating System Copilot[[paper](https://arxiv.org/abs/2404.18074)]
45. [2024/04/25] Benchmarking mobile device control agents across diverse configurations[[paper](https://arxiv.org/abs/2404.16660)]
46. [2024/04/15] MMInA: Benchmarking multihop multimodal internet agents[[paper](https://arxiv.org/abs/2404.09992)]
47. [2024/04/12] Llamatouch: A faithful and scalable testbed for mobile ui task automation[[paper](https://arxiv.org/abs/2404.16054)]
48. [2024/04/11] OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments[[paper](https://arxiv.org/abs/2404.07972)]
49. [2024/04/09] VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?[[paper](https://arxiv.org/abs/2404.05955)]
50. [2024/04/09] GUIDE: Graphical User Interface Data for Execution[[paper](https://arxiv.org/abs/2404.16048)]
51. [2024/04/08] Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs[[paper](https://arxiv.org/abs/2404.05719)]
52. [2024/04/04] Autowebglm: Bootstrap and reinforce a large language model-based web navigating agent[[paper](https://arxiv.org/abs/2404.03648)]
53. [2024/03/29] Evaluating language model agents on realistic autonomous tasks[[paper](https://arxiv.org/abs/2312.11671)]
54. [2024/03/29] Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want[[paper](https://arxiv.org/abs/2403.20271)]
55. [2024/03/26] AgentStudio: A Toolkit for Building General Virtual Agents[[paper](https://arxiv.org/abs/2403.17918)]
56. [2024/03/18] Tur[k]ingBench: A Challenge Benchmark for Web Agents[[paper](https://arxiv.org/abs/2403.11905)]
57. [2024/03/15] Computer User Interface Understanding. A New Dataset and a Learning Framework[[paper](https://arxiv.org/abs/2403.10170)]
58. [2024/03/12] Workarena: How capable are web agents at solving common knowledge work tasks?[[paper](https://arxiv.org/abs/2403.07718)]
59. [2024/03/06] PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion[[paper](https://arxiv.org/abs/2403.03788)]
60. [2024/03/05] Android in the Zoo:Chain-of-Action-Thought for GUI Agents[[paper](https://arxiv.org/abs/2403.02713)]
61. [2024/02/27] Omniact: A dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web[[paper](https://arxiv.org/abs/2402.17553)]
62. [2024/02/23] On the Multi-turn Instruction Following for Conversational Web Agents[[paper](https://arxiv.org/abs/2402.15057)]
63. [2024/02/09] Understanding the weakness of large language model agents within a complex android environment[[paper](https://arxiv.org/abs/2402.06596)]
64. [2024/02/09] Understanding the weakness of large language model agents within a complex android environment[[paper](https://arxiv.org/abs/2402.06596)]
65. [2024/02/08] WebLINX: Real-World website navigation with Multi-Turn dialogue[[paper](https://arxiv.org/abs/2402.05930)]
66. [2024/01/29] Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception[[paper](https://arxiv.org/abs/2401.16158)]
67. [2024/01/25] Webvoyager: Building an end-to-end web agent with large multimodal models[[paper](https://arxiv.org/abs/2401.13919)]
68. [2024/01/25] GPTVoiceTasker: Advancing Multi-step Mobile Task Efficiency Through Dynamic Interface Exploration and Learning[[paper](https://arxiv.org/abs/2401.14268)]
69. [2024/01/24] Visualwebarena: Evaluating multimodal agents on realistic visual web tasks[[paper](https://arxiv.org/abs/2401.13649)]
70. [2024/01/24] Agentboard: An analytical evaluation board of multi-turn llm agents[[paper](https://arxiv.org/abs/2401.13178)]
71. [2024/01/24] Agentboard: An analytical evaluation board of multiturn LLM agents[[paper](https://arxiv.org/abs/2401.13178)]
72. [2024/01/17] Seeclick: Harnessing gui grounding for advanced visual gui agents[[paper](https://arxiv.org/abs/2401.10935)]
73. [2023/12/26] AutoTask: Executing Arbitrary Voice Commands by Exploring and Learning from Mobile GUI[[paper](https://arxiv.org/abs/2312.16062)]
74. [2023/12/25] WebVLN: Vision-and-Language Navigation on Websites[[paper](https://arxiv.org/abs/2312.15820)]
75. [2023/12/20] ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation[[paper](https://arxiv.org/abs/2312.13108)]
76. [2023/11/30] Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web[[paper](https://arxiv.org/abs/2311.18751)]
77. [2023/11/21] GAIA: a benchmark for general AI assistants[[paper](https://arxiv.org/abs/2311.12983)]
78. [2023/11/13] GPT-4V in wonderland: Large multimodal models for Zero-Shot smartphone GUI navigation[[paper](https://arxiv.org/abs/2311.07562)]
79. [2023/11/03] Pptc benchmark: Evaluating large language models for powerpoint task completion[[paper](https://arxiv.org/abs/2311.01767)]
80. [2023/10/16] OpenAgents: An Open Platform for Language Agents in the Wild[[paper](https://arxiv.org/abs/2310.10634)]
81. [2023/09/20] You Only Look at Screens:Multimodal Chain-of-Action Agents[[paper](https://arxiv.org/abs/2309.11436)]
82. [2023/08/29] AutoDroid: LLM-powered Task Automation in Android[[paper](https://arxiv.org/abs/2308.15272)]
83. [2023/08/07] Agentbench: Evaluating llms as agents[[paper](https://arxiv.org/abs/2308.03688)]
84. [2023/07/25] Webarena: A realistic web environment for building autonomous agents[[paper](https://arxiv.org/abs/2307.13854)]
85. [2023/07/19] Android in the wild: A large-scale dataset for android device control[[paper](https://arxiv.org/abs/2307.10088)]
86. [2023/06/09] Mind2Web: Towards a Generalist Agent for the Web[[paper](https://arxiv.org/abs/2306.06070)]
87. [2023/05/30] Sheetcopilot: Bringing software productivity to the next level through large language models[[paper](https://arxiv.org/abs/2305.19308)]
88. [2023/05/25] On the tool manipulation capability of open-source large language models[[paper](https://arxiv.org/abs/2305.16504)]
89. [2023/05/14] Mobile-env: A universal platform for training and evaluation of mobile interaction[[paper](https://arxiv.org/abs/2305.08144v1)]
90. [2023/05/14] Mobile-env: Building qualified evaluation benchmarks for llm-gui interaction[[paper](https://arxiv.org/abs/2305.08144)]
91. [2023/04/14] Droidbot-gpt: Gpt-powered ui automation for android[[paper](https://arxiv.org/abs/2304.07061)]
92. [2023/04/10] OpenAGI: When LLM Meets Domain Experts[[paper](https://arxiv.org/abs/2304.04370)]
93. [2023/03/13] Vision-Language models as success detectors[[paper](https://arxiv.org/abs/2303.07280)]
94. [2022/11/14] Ugif: Ui grounded instruction following[[paper](https://arxiv.org/abs/2211.07615)]
95. [2022/10/08] Understanding html with large language models[[paper](https://arxiv.org/abs/2210.03945)]
96. [2022/09/29] MUG: Interactive Multimodal Grounding on User Interfaces[[paper](https://arxiv.org/abs/2209.15099)]
97. [2022/09/16] ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots[[paper](https://arxiv.org/abs/2209.08199)]
98. [2022/07/04] Webshop: Towards scalable real-world web interaction with grounded language agents[[paper](https://arxiv.org/abs/2207.01206)]
99. [2022/05/23] Meta-gui: Towards multi-modal conversational agents on mobile gui[[paper](https://arxiv.org/abs/2205.11029)]
100. [2022/02/04] A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility[[paper](https://arxiv.org/abs/2202.02312)]
101. [2021/04/17] Mobile app tasks with iterative feedback (motif): Addressing task feasibility in interactive visual environments[[paper](https://arxiv.org/abs/2104.08560)]
102. [2021/01/23] Websrc: A dataset for web-based structural reading comprehension[[paper](https://arxiv.org/abs/2101.09465)]
103. [2018/08/28] Mapping natural language commands to web elements[[paper](https://arxiv.org/abs/1808.09132)]
104. [2017/11/06] Building natural language interfaces to web apis[[paper](https://dl.acm.org/doi/10.1145/3132847.3133009)]
105. [2017/08/06] World of bits: An open-domain platform for web-based agents[[paper](https://dl.acm.org/doi/10.5555/3305890.3306005)]

### Safety & Privacy

### RL

1. [2024/10/18] DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agents [[paper](https://arxiv.org/abs/2410.14803)]
2. [2024/06/17] DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning [[paper](https://openreview.net/forum?id=Fqbg7yohJ9)]
3. [2024/05/23] AGILE: A Novel Reinforcement Learning Framework of LLM Agents [[paper](https://arxiv.org/abs/2405.14751)]
4. [2024/04/16] Search Beyond Queries: Training Smaller Language Models for Web Interactions via Reinforcement Learning [[paper](https://arxiv.org/abs/2404.10887)]
5. [2021/05/31] Appbuddy: Learning to accomplish tasks in mobile apps via reinforcement learning [[paper](https://arxiv.org/abs/2106.00133)]
6. [2019/02/19] Dom-q-net: Grounded rl on structured language [[paper](https://arxiv.org/abs/1902.07257)]
7. [2018/12/21] Learning to navigate the web [[paper](https://arxiv.org/abs/1812.09195)]
8. [2018/02/24] Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration [[paper](https://arxiv.org/abs/1802.08802)]
9. [2017/08/06] World of bits: An open-domain platform for web-based agents [[paper](https://proceedings.mlr.press/v70/shi17a/shi17a.pdf)]

## Related Repositories

## Contributing

## Citation

## Acknowledgement
