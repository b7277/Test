# Test
## Papers
### Foundation Models
1. [2024/10/31] AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents[[paper](https://arxiv.org/abs/2410.24024)]
2. [2024/10/30] OS-ATLAS: A Foundation Action Model for Generalist GUI Agents[[paper](https://arxiv.org/abs/2410.23218)]
3. [2024/10/28] AutoGLM: Autonomous Foundation Agents for GUIs[[paper](https://arxiv.org/abs/2411.00820)]
4. [2024/10/25] EDGE: Enhanced Grounded GUI Understanding with Enriched Multi-Granularity Synthetic Data[[paper](https://arxiv.org/abs/2410.19461)]
5. [2024/10/24] Ferret-UI One: Mastering Universal User Interface Understanding Across Platforms[[paper](https://arxiv.org/abs/2410.18967)]
6. [2024/10/22] ShowUI: One Vision-Language-Action Model for Generalist GUI Agent[[paper](https://openreview.net/forum?id=UXdxYnkJtX)]
7. [2024/10/17] Harnessing Webpage UIs for Text-Rich Visual Understanding[[paper](https://arxiv.org/abs/2410.13824)]
8. [2024/10/09] TinyClick: Single-Turn Agent for Empowering GUI Automation[[paper](https://arxiv.org/abs/2410.11871)]
9. [2024/10/07] Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents[[paper](https://arxiv.org/abs/2410.05243)]
10. [2024/10/07] Navigating The Digital World As Humans Do: Universal Visual Grounding For Gui Agents[[paper](https://arxiv.org/abs/2410.05243)]
11. [2024/10/03] NNetscape Navigator: Complex Demonstrations for Web Agents Without a Demonstrator[[paper](https://arxiv.org/abs/2410.02907)]
12. [2024/09/30] MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning[[paper](https://arxiv.org/abs/2409.20566)]
13. [2024/09/24] Synatra: Turning indirect knowledge into direct demonstrations for digital agents at scale[[paper](https://arxiv.org/abs/2409.15637)]
14. [2024/09/23] MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding[[paper](https://arxiv.org/abs/2409.14818)]
15. [2024/09/22] MobileViews: A Large-Scale Mobile GUI Dataset[[paper](https://arxiv.org/abs/2409.14337)]
16. [2024/08/30] UI-Hawk: Unleashing the screen stream understanding for gui agents[[paper](https://www.preprints.org/manuscript/202408.2137)]
17. [2024/07/19] GUI Action Narrator: Where and When Did That Action Take Place?[[paper](https://arxiv.org/abs/2406.13719)]
18. [2024/07/05] MobileFlow: A Multimodal LLM for Mobile GUI Agent[[paper](https://arxiv.org/abs/2407.04346)]
19. [2024/06/20] VGA: Vision GUI Assistant - Minimizing Hallucinations through Image-Centric Fine-Tuning[[paper](https://arxiv.org/abs/2406.14056)]
20. [2024/06/12] GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices[[paper](https://arxiv.org/abs/2406.08451)]
21. [2024/06/12] Tell Me What's Next: Textual Foresight for Generic UI Representations[[paper](https://arxiv.org/abs/2406.07822)]
22. [2024/05/29] ReALM: Reference Resolution As Language Modeling[[paper](https://arxiv.org/abs/2403.20329)]
23. [2024/05/08] Visual Grounding for User Interfaces[[paper](https://aclanthology.org/2024.naacl-industry.9/)]
24. [2024/05/05] Visual grounding for desktop graphical user interfaces[[paper](https://arxiv.org/abs/2407.01558)]
25. [2024/05/05] Android in the Zooï¼šChain-of-Action-Thought for GUI Agents[[paper](https://arxiv.org/abs/2403.02713)]
26. [2024/05/01] Navigating WebAI: Training Agents to Complete Web Tasks with Large Language Models and Reinforcement Learning[[paper](https://arxiv.org/abs/2405.00516)]
27. [2024/04/16] Search Beyond Queries: Training Smaller Language Models for Web Interactions via Reinforcement Learning[[paper](https://arxiv.org/abs/2404.10887)]
28. [2024/04/12] Training a Vision Language Model as Smartphone Assistant[[paper](https://arxiv.org/abs/2404.08755)]
29. [2024/04/09] Autonomous Evaluation and Refinement of Web Agents[[paper](https://arxiv.org/abs/2404.06474)]
30. [2024/04/08] Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs[[paper](https://arxiv.org/abs/2404.05719)]
31. [2024/04/04] AutoWebGLM: A Large Language Model-based Web Navigating Agent[[paper](https://arxiv.org/abs/2404.03648)]
32. [2024/04/04] AutoWebGLM: A Large Language Model-based Web Navigating Agent[[paper](https://arxiv.org/abs/2404.03648)]
33. [2024/04/02] Octopus v2: On-device language model for super agent[[paper](https://arxiv.org/abs/2404.01744)]
34. [2024/03/30] Large Language Models Can Self-Improve At Web Agent Tasks[[paper](https://arxiv.org/abs/2405.20309v2)]
35. [2024/02/08] WebLINX: Real-World website navigation with Multi-Turn dialogue[[paper](https://arxiv.org/abs/2402.05930)]
36. [2024/02/07] ScreenAI: A Vision-Language Model for UI and Infographics Understanding[[paper](https://arxiv.org/abs/2402.04615)]
37. [2024/02/06] Dual-View Visual Contextualization for Web Navigation[[paper](https://arxiv.org/abs/2402.04476)]
38. [2024/01/20] E-ANT: A Large-Scale Dataset for Efficient Automatic GUI NavigaTion[[paper](https://arxiv.org/abs/2406.14250)]
39. [2024/01/17] SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents[[paper](https://arxiv.org/abs/2401.10935)]
40. [2024/01/17] GUICourse: From General Vision Language Models to Versatile GUI Agents[[paper](https://arxiv.org/abs/2406.11317)]
41. [2023/12/25] WebVLN: Vision-and-Language Navigation on Websites[[paper](https://arxiv.org/abs/2312.15820)]
42. [2023/12/25] UINav: A Practical Approach to Train On-Device Automation Agents[[paper](https://aclanthology.org/2024.naacl-industry.4/)]
43. [2023/12/14] CogAgent: A Visual Language Model for GUI Agents[[paper](https://arxiv.org/abs/2312.08914)]
44. [2023/12/04] Intelligent Virtual Assistants with LLM-based Process Automation[[paper](https://arxiv.org/abs/2312.06677)]
45. [2023/11/30] Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web[[paper](https://arxiv.org/abs/2311.18751)]
46. [2023/10/27] Android in the wild: A large-scale dataset for android device control[[paper](https://arxiv.org/abs/2307.10088)]
47. [2023/10/08] UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model[[paper](https://arxiv.org/abs/2310.05126)]
48. [2023/10/07] ILuvUI: Instruction-tuned Language-Vision modeling of UIs from Machine Conversations[[paper](https://arxiv.org/abs/2310.04869)]
49. [2023/10/07] Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API[[paper](https://arxiv.org/abs/2310.04716)]
50. [2023/07/24] A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis[[paper](https://arxiv.org/abs/2307.12856)]
51. [2023/05/31] From pixels to UI actions: Learning to follow instructions via graphical user interfaces[[paper](https://arxiv.org/abs/2306.00245)]
52. [2023/05/19] Multimodal Web Navigation with Instruction-Finetuned Foundation Models[[paper](https://arxiv.org/abs/2305.11854)]
53. [2023/01/30] WebUI: A Dataset for Enhancing Visual UI Understanding with Web Semantics[[paper](https://arxiv.org/abs/2301.13280)]
54. [2023/01/23] Lexi: Self-Supervised Learning of the UI Language[[paper](https://arxiv.org/abs/2301.10165)]
55. [2022/10/06] Towards Better Semantic Understanding of Mobile Interfaces[[paper](https://arxiv.org/abs/2210.02663)]
56. [2022/09/29] Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus[[paper](https://arxiv.org/abs/2209.14927)]
57. [2022/07/04] WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents[[paper](https://arxiv.org/abs/2207.01206)]
58. [2022/05/23] Meta-gui: Towards multi-modal conversational agents on mobile gui[[paper](https://arxiv.org/abs/2205.11029)]
59. [2022/02/16] A data-driven approach for learning to control computers[[paper](https://arxiv.org/abs/2202.08137)]

### Agent Frameworks
1. [2024/11/10] Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents[[paper](https://arxiv.org/abs/2411.06559)]
2. [2024/11/01] WebOlympus: An Open Platform for Web Agents on Live Websites[[paper](https://aclanthology.org/2024.emnlp-demo.20/)]
3. [2024/10/29] Auto-Intent: Automated Intent Discovery and Self-Exploration for Large Language Model Web Agents[[paper](https://arxiv.org/abs/2410.22552)]
4. [2024/10/25] OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization[[paper](https://arxiv.org/abs/2410.19609)]
5. [2024/10/24] OSCAR: Operating System Control via State-Aware Reasoning and Re-Planning[[paper](https://arxiv.org/abs/2410.18963)]
6. [2024/10/24] AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant[[paper](https://arxiv.org/abs/2410.18603)]
7. [2024/10/24] Infogent: An Agent-Based Framework for Web Information Aggregation[[paper](https://arxiv.org/abs/2410.19054)]
8. [2024/10/22] Large Language Models Empowered Personalized Web Agents[[paper](https://arxiv.org/abs/2410.17236)]
9. [2024/10/21] Beyond Browsing: API-Based Web Agents[[paper](https://arxiv.org/abs/2410.16464)]
10. [2024/10/17] AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents[[paper](https://arxiv.org/abs/2410.13825)]
11. [2024/10/17] MobA: A Two-Level Agent System for Efficient Mobile Task Automation[[paper](https://arxiv.org/abs/2410.13757)]
12. [2024/10/11] VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning[[paper](https://dl.acm.org/doi/abs/10.1145/3654777.3676386)]
13. [2024/10/10] Agent S: An Open Agentic Framework that Uses Computers Like a Human[[paper](https://arxiv.org/abs/2410.08164)]
14. [2024/10/09] ClickAgent: Enhancing UI Location Capabilities of Autonomous Agents[[paper](https://arxiv.org/abs/2410.11872)]
15. [2024/10/01] Dynamic Planning for LLM-based Graphical User Interface Automation[[paper](https://arxiv.org/abs/2410.00467)]
16. [2024/10/01] Multimodal Auto Validation For Self-Refinement in Web Agents[[paper](https://arxiv.org/abs/2410.00689)]
17. [2024/09/25] Turn Every Application into an Agent: Towards Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents[[paper](https://arxiv.org/abs/2409.17140)]
18. [2024/09/23] From Commands to Prompts: LLM-based Semantic File System for AIOS[[paper](https://arxiv.org/abs/2410.11843)]
19. [2024/09/23] Steward: Natural Language Web Automation[[paper](https://arxiv.org/abs/2409.15441)]
20. [2024/09/16] NaviQAte: Functionality-Guided Web Application Navigation[[paper](https://arxiv.org/abs/2409.10741)]
21. [2024/09/14] PeriGuru: A Peripheral Robotic Mobile App Operation Assistant based on GUI Image Understanding and Prompting with LLM[[paper](https://arxiv.org/abs/2409.09354)]
22. [2024/09/12] Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale[[paper](https://arxiv.org/abs/2409.08264)]
23. [2024/09/11] Agent Workflow Memory[[paper](https://arxiv.org/abs/2409.07429)]
24. [2024/08/28] Webpilot: A versatile and autonomous multi-agent system for web task execution with strategic exploration[[paper](https://arxiv.org/abs/2408.15978)]
25. [2024/08/24] AutoWebGLM: A Large Language Model-based Web Navigating Agent[[paper](https://dl.acm.org/doi/abs/10.1145/3637528.3671620)]
26. [2024/08/24] Intelligent Agents with LLM-based Process Automation[[paper](https://dl.acm.org/doi/abs/10.1145/3637528.3671646)]
27. [2024/08/01] OpenWebAgent: An Open Toolkit to Enable Web Agents on Large Language Models[[paper](https://aclanthology.org/2024.acl-demos.8/)]
28. [2024/08/01] Omniparser for pure vision based gui agent[[paper](https://arxiv.org/abs/2408.00203)]
29. [2024/07/21] Towards LLMCI: Multimodal AI for LLM-Vision UI Operation[[paper](https://doi.org/10.21203/rs.3.rs-4653823/v1)]
30. [2024/07/17] Agent-e: From autonomous web navigation to foundational design principles in agentic systems[[paper](https://arxiv.org/abs/2407.13032)]
31. [2024/07/04] MobileExperts: A Dynamic Tool-Enabled Agent Team in Mobile Devices[[paper](https://arxiv.org/abs/2407.03913)]
32. [2024/07/01] Tree Search for Language Model Agents[[paper](https://arxiv.org/abs/2407.01476)]
33. [2024/06/27] Read anywhere pointed: Layout-aware gui screen reading with tree-of-lens grounding[[paper](https://arxiv.org/abs/2406.19263)]
34. [2024/06/11] CAAP: Context-Aware Action Planning Prompting to Solve Computer Tasks with Front-End UI Only[[paper](https://arxiv.org/abs/2406.06947)]
35. [2024/06/03] Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration[[paper](https://arxiv.org/abs/2406.01014)]
36. [2024/05/23] AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents[[paper](https://arxiv.org/abs/2405.14573)]
37. [2024/05/17] Latent State Estimation Helps UI Agents to Reason[[paper](https://arxiv.org/abs/2405.11120)]
38. [2024/04/28] MMAC-Copilot: Multi-modal Agent Collaboration Operating System Copilot[[paper](https://arxiv.org/abs/2404.18074)]
39. [2024/04/09] Autonomous Evaluation and Refinement of Web Agents[[paper](https://arxiv.org/abs/2404.06474)]
40. [2024/04/03] PromptRPA: Generating Robotic Process Automation on Smartphones from Textual Prompts[[paper](https://arxiv.org/abs/2404.02475)]
41. [2024/03/25] AIOS: LLM Agent Operating System[[paper](https://arxiv.org/abs/2403.16971v3)]
42. [2024/03/05] Android in the zoo: Chain-of-action-thought for gui agents[[paper](https://arxiv.org/abs/2403.02713)]
43. [2024/03/05] Cradle: Empowering Foundation Agents Towards General Computer Control[[paper](https://arxiv.org/abs/2403.03186v3)]
44. [2024/02/23] On the Multi-turn Instruction Following for Conversational Web Agents[[paper](https://arxiv.org/abs/2402.15057)]
45. [2024/02/19] CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation[[paper](https://arxiv.org/abs/2402.11941)]
46. [2024/02/12] OS-Copilot: Towards Generalist Computer Agents with Self-Improvement[[paper](https://arxiv.org/abs/2402.07456)]
47. [2024/02/09] ScreenAgent: A Vision Language Model-driven Computer Control Agent[[paper](https://arxiv.org/abs/2402.07945)]
48. [2024/02/08] Ufo: A ui-focused agent for windows os interaction[[paper](https://arxiv.org/abs/2402.07939)]
49. [2024/02/06] Dual-View Visual Contextualization for Web Navigation[[paper](https://arxiv.org/abs/2402.04476)]
50. [2024/01/29] Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception[[paper](https://arxiv.org/abs/2401.16158)]
51. [2024/01/25] WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models[[paper](https://arxiv.org/abs/2401.13919)]
52. [2024/01/17] Seeclick: Harnessing gui grounding for advanced visual gui agents[[paper](https://arxiv.org/abs/2401.10935)]
53. [2024/01/04] MobileAgent: enhancing mobile control via human-machine interaction and SOP integration[[paper](https://arxiv.org/abs/2401.04124)]
54. [2024/01/03] GPT-4V(ision) is a Generalist Web Agent, if Grounded[[paper](https://arxiv.org/abs/2401.01614)]
55. [2023/12/21] AppAgent: Multimodal Agents as Smartphone Users[[paper](https://arxiv.org/abs/2312.13771)]
56. [2023/12/20] Assistgui: Task-oriented desktop graphical user interface automation[[paper](https://arxiv.org/abs/2312.13108)]
57. [2023/12/04] MobileGPT: Augmenting LLM with Human-like App Memory for Mobile Task Automation[[paper](https://arxiv.org/abs/2312.03003v3)]
58. [2023/12/04] Intelligent Virtual Assistants with LLM-based Process Automation[[paper](https://arxiv.org/abs/2312.06677)]
59. [2023/11/13] GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation[[paper](https://arxiv.org/abs/2311.07562)]
60. [2023/10/24] WebWISE: Web Interface Control and Sequential Exploration with Large Language Models[[paper](https://arxiv.org/abs/2310.16042)]
61. [2023/10/12] A Zero-Shot Language Agent for Computer Control with Structured Reflection[[paper](https://arxiv.org/abs/2310.08740)]
62. [2023/09/20] You only look at screens: Multimodal chain-of-action agents[[paper](https://arxiv.org/abs/2309.11436)]
63. [2023/09/15] Laser: Llm agent with state-space exploration for web navigation[[paper](https://arxiv.org/abs/2309.08172)]
64. [2023/08/29] AutoDroid: LLM-powered Task Automation in Android[[paper](https://arxiv.org/abs/2308.15272)]
65. [2023/07/24] A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis[[paper](https://arxiv.org/abs/2307.12856)]
66. [2023/06/14] Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control[[paper](https://arxiv.org/abs/2306.07863)]
67. [2023/06/09] Mind2Web: Towards a Generalist Agent for the Web[[paper](https://arxiv.org/abs/2306.06070)]
68. [2023/05/30] Sheetcopilot: Bringing software productivity to the next level through large language models[[paper](https://arxiv.org/abs/2305.19308)]
69. [2023/05/23] Hierarchical prompting assists large language model on web navigation[[paper](https://arxiv.org/abs/2305.14257)]
70. [2023/05/09] InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language[[paper](https://arxiv.org/abs/2305.05662)]
71. [2023/03/30] Language Models can Solve Computer Tasks[[paper](https://arxiv.org/abs/2303.17491)]
72. [2022/09/19] Enabling conversational interaction with mobile ui using large language models[[paper](https://arxiv.org/abs/2209.08655)]
73. [2022/05/23] Meta-gui: Towards multi-modal conversational agents on mobile gui[[paper](https://arxiv.org/abs/2205.11029)]
### Evaluation & Benchmark
### Safety & Privacy
### RL
1. [2024/10/18] DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agents [[paper](https://arxiv.org/abs/2410.14803)]
2. [2024/06/17] DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning [[paper](https://openreview.net/forum?id=Fqbg7yohJ9)]
3. [2024/05/23] AGILE: A Novel Reinforcement Learning Framework of LLM Agents [[paper](https://arxiv.org/abs/2405.14751)]
4. [2024/04/16] Search Beyond Queries: Training Smaller Language Models for Web Interactions via Reinforcement Learning [[paper](https://arxiv.org/abs/2404.10887)]
5. [2021/05/31] Appbuddy: Learning to accomplish tasks in mobile apps via reinforcement learning [[paper](https://arxiv.org/abs/2106.00133)]
6. [2019/02/19] Dom-q-net: Grounded rl on structured language [[paper](https://arxiv.org/abs/1902.07257)]
7. [2018/12/21] Learning to navigate the web [[paper](https://arxiv.org/abs/1812.09195)]
8. [2018/02/24] Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration [[paper](https://arxiv.org/abs/1802.08802)]
9. [2017/08/06] World of bits: An open-domain platform for web-based agents [[paper](https://proceedings.mlr.press/v70/shi17a/shi17a.pdf)]
